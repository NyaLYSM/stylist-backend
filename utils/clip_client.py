# utils/clip_client.py
import logging
import requests
from PIL import Image

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
CLIP_SERVICE_URL = "http://127.0.0.1:8001"
# –ú–æ–¥–µ–ª—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ—Å—É—Ä—Å—ã)
HF_MODEL_NAME = "openai/clip-vit-base-patch32"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (Lazy Loading)
_model = None
_processor = None
_device = None
_clip_loaded = False

def init_local_clip():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç CLIP –≤ –ø–∞–º—è—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–∞, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ transformers –∏ torch.
    –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å —Å–∫–æ—Ä–∏–Ω–≥ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    """
    global _model, _processor, _device, _clip_loaded
    
    if _clip_loaded:
        return True

    try:
        import torch
        from transformers import CLIPProcessor, CLIPModel
        
        logger.info("üß† Loading CLIP model locally...")
        _device = "cuda" if torch.cuda.is_available() else "cpu"
        _model = CLIPModel.from_pretrained(HF_MODEL_NAME).to(_device)
        _processor = CLIPProcessor.from_pretrained(HF_MODEL_NAME)
        _model.eval() # –†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        _clip_loaded = True
        logger.info(f"‚úÖ CLIP loaded on {_device}")
        return True
    except ImportError:
        logger.warning("‚ö†Ô∏è Transformers/Torch not installed. CLIP scoring will be disabled.")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error loading CLIP: {e}")
        return False

def rate_image_relevance(image: Image.Image, product_name: str) -> float:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç (0-100), –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–∞,
    –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—è –º—É—Å–æ—Ä (—Ç–∞–±–ª–∏—Ü—ã, —É–ø–∞–∫–æ–≤–∫—É, —Å–ª–æ–∂–Ω—ã–µ –∞—É—Ç—Ñ–∏—Ç—ã).
    """
    # 1. –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å
    if not _clip_loaded:
        if not init_local_clip():
            return 50.0 # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, –µ—Å–ª–∏ CLIP –Ω–µ—Ç

    try:
        import torch
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π CLIP –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Ç—Ä–∞–Ω—Å–ª–∏—Ç –∏–ª–∏ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã, 
        # –Ω–æ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π —Å–ø—Ä–∞–≤–∏—Ç—Å—è –∏ —Å —Ä—É—Å—Å–∫–∏–º. –î–ª—è –±–∞–∑—ã openai –ª—É—á—à–µ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç).
        # –ü—Ä–æ—Å—Ç–æ–π —Ö–∞–∫: –¥–æ–±–∞–≤–ª—è–µ–º "clothing item" —á—Ç–æ–±—ã –∑–∞–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        # –ü–û–ó–ò–¢–ò–í–ù–´–ï –ò –ù–ï–ì–ê–¢–ò–í–ù–´–ï –ö–õ–ê–°–°–´
        # CLIP —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: "–ù–∞ —á—Ç–æ —ç—Ç–æ –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–µ?"
        choices = [
            f"photo of {product_name}, product view, clean background", # 0: –¢–æ, —á—Ç–æ –∏—â–µ–º (Target)
            "size chart, text table, infographics with numbers",        # 1: –¢–∞–±–ª–∏—Ü—ã (–ú—É—Å–æ—Ä)
            "close-up fabric texture, macro shot",                      # 2: –¢–µ–∫—Å—Ç—É—Ä—ã (–ú—É—Å–æ—Ä)
            "packaging box, plastic bag, delivery package",             # 3: –£–ø–∞–∫–æ–≤–∫–∞ (–ú—É—Å–æ—Ä)
            "full body outfit, messy background, street style, many items" # 4: –ê—É—Ç—Ñ–∏—Ç (–≥–¥–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ –ø—Ä–æ–¥–∞–µ–º)
        ]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        inputs = _processor(
            text=choices, 
            images=image, 
            return_tensors="pt", 
            padding=True,
            truncation=True
        ).to(_device)

        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        with torch.no_grad():
            outputs = _model(**inputs)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (softmax)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1) # shape: [1, 5]
        
        # Score = –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —ç—Ç–æ –Ω–∞—à —Ç–æ–≤–∞—Ä (–∏–Ω–¥–µ–∫—Å 0)
        # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100 –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        target_prob = probs[0][0].item()
        chart_prob = probs[0][1].item()
        
        # –î–æ–ø. –ª–æ–≥–∏–∫–∞: –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "—Ç–∞–±–ª–∏—Ü—ã" (–∏–Ω–¥–µ–∫—Å 1) –≤—ã—à–µ 10%, —Å–∏–ª—å–Ω–æ —à—Ç—Ä–∞—Ñ—É–µ–º
        if chart_prob > 0.1:
            return 10.0
            
        return target_prob * 100.0

    except Exception as e:
        logger.error(f"‚ö†Ô∏è CLIP scoring failed: {e}")
        return 50.0

# --- –°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (HTTP) ---

def clip_check_clothing(image_url: str) -> dict:
    """–û—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –∑–∞–ø—Ä–æ—Å–∞ –∫ –≤–Ω–µ—à–Ω–µ–º—É —Å–µ—Ä–≤–∏—Å—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω—É–∂–Ω–∞"""
    try:
        r = requests.post(
            f"{CLIP_SERVICE_URL}/check-clothing",
            json={"image_url": image_url},
            timeout=5
        )
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return {"ok": True} # Fallback
