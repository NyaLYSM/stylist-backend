import os
import uuid
import time
import asyncio
import re
import logging
import json
from datetime import datetime
from io import BytesIO
from PIL import Image, ImageStat, ImageFilter

import requests
import concurrent.futures
from curl_cffi import requests as crequests
from bs4 import BeautifulSoup

from fastapi import APIRouter, Depends, UploadFile, HTTPException, File, Form
from pydantic import BaseModel
from sqlalchemy.orm import Session

from database import get_db
from models import WardrobeItem
from utils.storage import delete_image, save_image
from utils.validators import validate_name
from .dependencies import get_current_user_id

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LOGGER ===
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === –ò–ú–ü–û–†–¢ ML –ú–û–î–£–õ–ï–ô ===
CLIP_AVAILABLE = False
try:
    from utils.clip_client import clip_check_clothing, rate_image_relevance
    CLIP_AVAILABLE = True
    logger.info("‚úÖ CLIP client module loaded")
except ImportError:
    logger.warning("‚ö†Ô∏è CLIP module not found. Smart sorting disabled.")
    def rate_image_relevance(img, name): return 50.0

router = APIRouter(tags=["Wardrobe"])

# --- Models ---
class ItemUrlPayload(BaseModel):
    name: str
    url: str

class ItemResponse(BaseModel):
    id: int
    name: str
    image_url: str
    item_type: str
    created_at: datetime
    class Config:
        from_attributes = True

class SelectVariantPayload(BaseModel):
    temp_id: str
    selected_variant: str
    name: str

VARIANTS_STORAGE = {}

# --- Smart Title Extraction ---
def extract_smart_title(full_title: str) -> str:
    """
    –ß–∏—Å—Ç–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å—É—Ç—å –¥–ª—è CLIP.
    –ü—Ä–∏–º–µ—Ä: "–ü–ª–∞—Ç—å–µ –∂–µ–Ω—Å–∫–æ–µ –≤–µ—á–µ—Ä–Ω–µ–µ —á–µ—Ä–Ω–æ–µ –æ–≤–µ—Ä—Å–∞–π–∑ - –∫—É–ø–∏—Ç—å..." -> "–ü–ª–∞—Ç—å–µ –≤–µ—á–µ—Ä–Ω–µ–µ —á–µ—Ä–Ω–æ–µ"
    """
    if not full_title: return "clothing"
    
    # 1. –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞ –º–∞–≥–∞–∑–∏–Ω–æ–≤
    cleanup_patterns = [
        r'[-|‚Äì].*wildberries.*', r'[-|‚Äì].*ozon.*', r'[-|‚Äì].*lamoda.*',
        r'–∫—É–ø–∏—Ç—å –≤ .*', r'–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω.*', r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç.*',
        r'wildberries', 'wb', 'ozon', 'lamoda', 'aliexpress'
    ]
    
    title = full_title.lower()
    for pat in cleanup_patterns:
        title = re.sub(pat, '', title)

    # 2. –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—à—É–º)
    stop_words = [
        '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–Ω–æ–≤–∏–Ω–∫–∞', '—Ö–∏—Ç', 'new', 'sale',
        '–±—ã—Å—Ç—Ä–∞—è', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è', '–∂–µ–Ω—Å–∫–∏–µ', '–º—É–∂—Å–∫–∏–µ', '–¥–µ—Ç—Å–∫–∏–µ',
        '–¥–ª—è', '–∂–µ–Ω—â–∏–Ω', '–º—É–∂—á–∏–Ω', '–¥–µ–≤–æ—á–µ–∫', '–º–∞–ª—å—á–∏–∫–æ–≤', '–æ–¥–µ–∂–¥–∞',
        '—Ä–∞–∑–º–µ—Ä', '—Ü–≤–µ—Ç', '–∞—Ä—Ç–∏–∫—É–ª', '—à—Ç', '—É–ø'
    ]
    
    for w in stop_words:
        title = re.sub(rf'\b{w}\b', '', title)
        
    # 3. –§–∏–Ω–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞
    title = re.sub(r'[^\w\s]', ' ', title) # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    title = re.sub(r'\s+', ' ', title).strip() # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5-6 —Å–ª–æ–≤, –æ–±—ã—á–Ω–æ —ç—Ç–æ "–°—É—Ç—å + –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
    words = title.split()
    if not words: return "clothing"
    
    result = ' '.join(words[:6])
    return result.capitalize()

# --- Image Tools ---
def is_valid_image_url(url: str) -> bool:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —è–≤–Ω—ã–π –º—É—Å–æ—Ä –≤ URL"""
    if not url or not url.startswith('http'): return False
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏–∫–æ–Ω–∫–∏, svg, gif (–æ–±—ã—á–Ω–æ –ª–æ–∞–¥–µ—Ä—ã)
    if any(x in url.lower() for x in ['.svg', '.gif', 'icon', 'logo', 'loader', 'blank']):
        return False
    return True

def analyze_image_score(img: Image.Image, index: int, total_images: int) -> float:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≠–≤—Ä–∏—Å—Ç–∏–∫–∞)"""
    score = 100.0
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–∑–∏—Ü–∏—é (—á–µ–º –¥–∞–ª—å—à–µ, —Ç–µ–º –º–µ–Ω—å—à–µ —à–∞–Ω—Å, —á—Ç–æ —ç—Ç–æ —Ö–æ—Ä–æ—à–µ–µ —Ñ–æ—Ç–æ)
    if index > 2: score -= (index * 5)
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ —Ä–∞–∑–º–µ—Ä (—Å–ª–∏—à–∫–æ–º –º–µ–ª–∫–∏–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –≤—ã—Ç—è–Ω—É—Ç—ã–µ –±–∞–Ω–Ω–µ—Ä—ã)
    w, h = img.size
    if w < 300 or h < 300: score -= 50
    aspect = w / h
    if aspect > 1.8 or aspect < 0.4: score -= 30 # –ë–∞–Ω–Ω–µ—Ä—ã
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ "—à—É–º" (—Ç–µ–∫—Å—Ç/—Ç–∞–±–ª–∏—Ü—ã)
    gray = img.convert("L")
    edges = gray.filter(ImageFilter.FIND_EDGES)
    edge_density = ImageStat.Stat(edges).mean[0]
    
    if edge_density > 50: 
        score -= 40 # –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ç–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏–ª–∏ —Ç–µ–∫—Å—Ç
        
    return max(0, score)

# --- MARKETPLACE PARSERS ---

def parse_wildberries(url: str, logger) -> tuple[list, str]:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Wildberries"""
    image_urls = []
    title = None
    nm_id = None
    
    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º ID
    match = re.search(r'catalog/(\d+)', url)
    if match: nm_id = int(match.group(1))
    
    if not nm_id: return [], None

    # 2. –°—Ç—Ä–∞—Ç–µ–≥–∏—è A: Mobile API
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º card.wb.ru
        api_url = f"https://card.wb.ru/cards/v2/detail?appType=1&curr=rub&dest=-1257786&spp=30&nm={nm_id}"
        headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"
        }
        resp = requests.get(api_url, headers=headers, timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            products = data.get('data', {}).get('products', [])
            if products:
                prod = products[0]
                title = prod.get('name')
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ (basket-01 ... basket-75)
                # –†–ê–°–®–ò–†–Ø–ï–ú –î–ò–ê–ü–ê–ó–û–ù –û–ë–†–ê–¢–ù–û, –∏–Ω–∞—á–µ –Ω–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥—É—Ç—Å—è!
                vol = nm_id // 100000
                part = nm_id // 1000
                hosts = [f"basket-{i:02d}.wbbasket.ru" for i in range(1, 76)] 
                
                # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—á–µ–≥–æ —Ö–æ—Å—Ç–∞ (Head request)
                found_host = None
                
                # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–∞ (—Ç–∞–º —á–∞—â–µ –Ω–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã)
                for h in reversed(hosts):
                    test_url = f"https://{h}/vol{vol}/part{part}/{nm_id}/images/big/1.webp"
                    try:
                        # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç, —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å
                        if requests.head(test_url, timeout=0.2).status_code == 200:
                            found_host = h
                            break
                    except: continue
                
                if found_host:
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Å—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
                    for i in range(1, 15):
                        image_urls.append(f"https://{found_host}/vol{vol}/part{part}/{nm_id}/images/big/{i}.webp")
                    
                    logger.info(f"‚úÖ WB Strategy Success: Host={found_host}")
                    return image_urls, title
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è WB API Strategy failed: {e}")

    # 3. Fallback
    return parse_generic_json_ld(url, logger)

def parse_generic_json_ld(url: str, logger) -> tuple[list, str]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Ozon, Lamoda –∏ –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤, 
    –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö Schema.org (JSON-LD) –∏–ª–∏ Open Graph.
    """
    image_urls = []
    title = None
    
    try:
        # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä
        resp = crequests.get(
            url, 
            impersonate="chrome120", 
            headers={"Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8"},
            timeout=10
        )
        
        if resp.status_code != 200: return [], None
        
        soup = BeautifulSoup(resp.content, "lxml")
        
        # A. –ò—â–µ–º JSON-LD (Schema.org) - –ó–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, list): data = data[0] # –ò–Ω–æ–≥–¥–∞ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                
                # –ò—â–µ–º –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞ Product
                if isinstance(data, dict):
                    # –ù–∞–∑–≤–∞–Ω–∏–µ
                    if not title and 'name' in data:
                        title = data['name']
                    
                    # –ö–∞—Ä—Ç–∏–Ω–∫–∏
                    if 'image' in data:
                        imgs = data['image']
                        if isinstance(imgs, str): image_urls.append(imgs)
                        elif isinstance(imgs, list): image_urls.extend(imgs)
            except: pass

        # B. –ò—â–µ–º Open Graph (og:title, og:image) - –°–µ—Ä–µ–±—Ä—è–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç
        if not title:
            og_title = soup.find("meta", property="og:title")
            if og_title: title = og_title.get("content")
            
        # –î–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ OG, –µ—Å–ª–∏ –ø—É—Å—Ç–æ
        if not image_urls:
            og_img = soup.find("meta", property="og:image")
            if og_img: image_urls.append(og_img.get("content"))

        # C. –ò—â–µ–º <title> –∏ <h1> - –ë—Ä–æ–Ω–∑–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç
        if not title:
            if soup.title: title = soup.title.string
            elif soup.find("h1"): title = soup.find("h1").get_text(strip=True)

        # D. "–ü—ã–ª–µ—Å–æ—Å–∏–º" –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ HTML, –µ—Å–ª–∏ —Å–æ–≤—Å–µ–º –ø—É—Å—Ç–æ (–¥–ª—è Lamoda/Ozon —á–∞—Å—Ç–æ –Ω—É–∂–Ω–æ)
        if len(image_urls) < 2:
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if is_valid_image_url(src):
                    # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞–∑–º–µ—Ä—É (–ø—Ä–æ—Å—Ç–µ–π—à–∏–π, –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º)
                    if 'icon' not in src and 'logo' not in src:
                        image_urls.append(src)

    except Exception as e:
        logger.error(f"‚ùå Generic Parser Error: {e}")
        
    # –ß–∏—Å—Ç–∏–º –¥—É–±–ª–∏
    image_urls = list(dict.fromkeys(image_urls))
    return image_urls[:15], title

# --- MAIN CONTROLLER ---

def get_marketplace_data(url: str):
    """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä: –≤—ã–±–∏—Ä–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å—Å—ã–ª–∫–∏"""
    logger.info(f"üåê Processing URL: {url}")
    
    if "wildberries" in url or "wb.ru" in url:
        return parse_wildberries(url, logger)
    
    elif "ozon" in url:
        return parse_generic_json_ld(url, logger)
        
    elif "lamoda" in url:
        return parse_generic_json_ld(url, logger)
        
    else:
        return parse_generic_json_ld(url, logger)

def download_image_bytes(image_url: str) -> bytes:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.google.com/'
    }
    try:
        resp = requests.get(image_url, headers=headers, timeout=15)
        if resp.status_code == 200:
            return resp.content
    except Exception as e:
        logger.warning(f"Download failed {image_url}: {e}")
    return None

# --- API ENDPOINTS ---

@router.post("/add-marketplace-with-variants")
async def add_marketplace_with_variants(
    payload: ItemUrlPayload, 
    db: Session = Depends(get_db), 
    user_id: int = Depends(get_current_user_id)
):
    loop = asyncio.get_event_loop()
    
    # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ ===
    # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫—É —Å–µ—Ä–≤–µ—Ä–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫—É –≤—ã–∑–æ–≤–∞
    try:
        image_urls, full_title = await loop.run_in_executor(
            None, 
            lambda: get_marketplace_data(payload.url)
        )
    except Exception as e:
        logger.error(f"‚ùå Parser crashed: {e}")
        raise HTTPException(400, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—Å—ã–ª–∫–∏: {str(e)}")

    if not image_urls:
        logger.warning(f"‚ùå No images found for {payload.url}")
        raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–æ—Ç–æ –≤—Ä—É—á–Ω—É—é.")

    # ... –î–∞–ª—å—à–µ –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ...
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Prompt –¥–ª—è CLIP
    raw_name = payload.name if payload.name else (full_title if full_title else "clothing")
    clip_prompt = extract_smart_title(raw_name)
    
    logger.info(f"üß† CLIP Search Prompt: '{clip_prompt}'")

    # 3. –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    temp_id = uuid.uuid4().hex
    candidates = []
    
    process_urls = image_urls[:10]
    
    for idx, img_url in enumerate(process_urls):
        try:
            file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(img_url))
            if not file_bytes: continue
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB
            img = Image.open(BytesIO(file_bytes))
            if img.mode != 'RGB':
                bg = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode in ('RGBA', 'LA'):
                    bg.paste(img, mask=img.split()[-1])
                else:
                    bg.paste(img)
                img = bg

            heuristic_score = analyze_image_score(img, idx, len(process_urls))
            
            clip_score = 0.0
            if CLIP_AVAILABLE and heuristic_score > 20: 
                clip_score = await loop.run_in_executor(
                    None,
                    lambda: rate_image_relevance(img, clip_prompt)
                )
            
            final_score = (heuristic_score * 0.3) + (clip_score * 0.7)
            
            preview_img = img.copy()
            preview_img.thumbnail((400, 400))
            out = BytesIO()
            preview_img.save(out, format='JPEG', quality=85)
            
            candidates.append({
                "score": final_score,
                "original_url": img_url,
                "preview_bytes": out.getvalue(),
                "original_idx": idx
            })
            
            logger.info(f"Img {idx+1}: Score={final_score:.1f} (CLIP={clip_score:.1f})")
            
        except Exception as e:
            logger.warning(f"Skipping img {idx}: {e}")

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    candidates.sort(key=lambda x: x["score"], reverse=True)
    top_candidates = candidates[:4]
    top_candidates.sort(key=lambda x: x["original_idx"])
    
    variant_previews = {}
    variant_full_urls = {}
    
    for cand in top_candidates:
        v_key = f"v_{cand['original_idx']}"
        fname = f"prev_{temp_id}_{v_key}.jpg"
        url = save_image(fname, cand['preview_bytes'])
        
        variant_previews[v_key] = url
        variant_full_urls[v_key] = cand['original_url']

    if not variant_previews:
         raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≤–æ–∑–º–æ–∂–Ω–æ, –∑–∞—â–∏—Ç–∞ —Å–∞–π—Ç–∞).")

    VARIANTS_STORAGE[temp_id] = {
        "image_urls": variant_full_urls,
        "previews": variant_previews,
        "user_id": user_id,
        "created_at": datetime.utcnow()
    }
    
    display_name = full_title if full_title else "–ù–æ–≤–∞—è –≤–µ—â—å"
    if len(display_name) > 60: display_name = display_name[:57] + "..."

    return {
        "temp_id": temp_id,
        "suggested_name": display_name,
        "variants": variant_previews,
        "total_images": len(variant_previews)
    }

@router.post("/select-variant", response_model=ItemResponse)
async def select_variant(
    payload: SelectVariantPayload,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    if payload.temp_id not in VARIANTS_STORAGE:
        raise HTTPException(404, "Session expired")
    
    data = VARIANTS_STORAGE[payload.temp_id]
    if data["user_id"] != user_id: raise HTTPException(403, "Access denied")
    
    target_url = data["image_urls"].get(payload.selected_variant)
    if not target_url: raise HTTPException(400, "Invalid variant")
    
    loop = asyncio.get_event_loop()
    file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(target_url))
    
    if not file_bytes: raise HTTPException(400, "Failed to download original")
    
    img = Image.open(BytesIO(file_bytes))
    if img.mode != 'RGB': img = img.convert('RGB') # Fix RGBA again just in case
    
    out = BytesIO()
    img.save(out, format='JPEG', quality=95)
    
    fname = f"item_{uuid.uuid4().hex}.jpg"
    final_url = save_image(fname, out.getvalue())
    
    # Cleanup
    for p_url in data["previews"].values():
        try: delete_image(p_url)
        except: pass
    del VARIANTS_STORAGE[payload.temp_id]
    
    item = WardrobeItem(
        user_id=user_id,
        name=payload.name,
        item_type="marketplace",
        image_url=final_url,
        created_at=datetime.utcnow()
    )
    db.add(item); db.commit(); db.refresh(item)
    return item

# --- –°—Ç–∞—Ä—ã–µ —Ä–æ—É—Ç—ã (–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏) ---
@router.get("/items", response_model=list[ItemResponse]) 
def get_wardrobe_items(user_id: int = Depends(get_current_user_id), db: Session = Depends(get_db)):
    items = db.query(WardrobeItem).filter(WardrobeItem.user_id == user_id).order_by(WardrobeItem.created_at.desc()).all()
    return items if items else []

@router.delete("/delete")
def delete_item(item_id: int, db: Session = Depends(get_db), user_id: int = Depends(get_current_user_id)):
    item = db.query(WardrobeItem).filter(WardrobeItem.id == item_id, WardrobeItem.user_id == user_id).first()
    if not item: raise HTTPException(404, "Not found")
    try: delete_image(item.image_url)
    except: pass
    db.delete(item); db.commit()
    return {"status": "success"}

