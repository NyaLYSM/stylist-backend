import os
import uuid
import time
import asyncio
import re
import logging
import json
from datetime import datetime
from io import BytesIO
from PIL import Image, ImageStat, ImageFilter

import requests
import concurrent.futures
from curl_cffi import requests as crequests
from bs4 import BeautifulSoup

from fastapi import APIRouter, Depends, UploadFile, HTTPException, File, Form
from pydantic import BaseModel
from sqlalchemy.orm import Session

from database import get_db
from models import WardrobeItem
from utils.storage import delete_image, save_image
from utils.validators import validate_name
from .dependencies import get_current_user_id

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LOGGER ===
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === –ò–ú–ü–û–†–¢ ML –ú–û–î–£–õ–ï–ô ===
CLIP_AVAILABLE = False
try:
    from utils.clip_client import clip_check_clothing, rate_image_relevance
    CLIP_AVAILABLE = True
    logger.info("‚úÖ CLIP client module loaded")
except ImportError:
    logger.warning("‚ö†Ô∏è CLIP module not found. Smart sorting disabled.")
    def rate_image_relevance(img, name): return 50.0

router = APIRouter(tags=["Wardrobe"])

# --- Models ---
class ItemUrlPayload(BaseModel):
    name: str
    url: str

class ItemResponse(BaseModel):
    id: int
    name: str
    image_url: str
    item_type: str
    created_at: datetime
    class Config:
        from_attributes = True

class SelectVariantPayload(BaseModel):
    temp_id: str
    selected_variant: str
    name: str

VARIANTS_STORAGE = {}

# --- Smart Title Extraction ---
def extract_smart_title(full_title: str) -> str:
    """–ß–∏—Å—Ç–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è CLIP"""
    if not full_title: return "clothing"
    
    cleanup_patterns = [
        r'[-|‚Äì].*wildberries.*', r'[-|‚Äì].*ozon.*', r'[-|‚Äì].*lamoda.*',
        r'–∫—É–ø–∏—Ç—å –≤ .*', r'–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω.*', r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç.*',
        r'wildberries', 'wb', 'ozon', 'lamoda', 'aliexpress'
    ]
    
    title = full_title.lower()
    for pat in cleanup_patterns:
        title = re.sub(pat, '', title)

    stop_words = [
        '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–Ω–æ–≤–∏–Ω–∫–∞', '—Ö–∏—Ç', 'new', 'sale',
        '–±—ã—Å—Ç—Ä–∞—è', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è', '–∂–µ–Ω—Å–∫–∏–µ', '–º—É–∂—Å–∫–∏–µ', '–¥–µ—Ç—Å–∫–∏–µ',
        '–¥–ª—è', '–∂–µ–Ω—â–∏–Ω', '–º—É–∂—á–∏–Ω', '–¥–µ–≤–æ—á–µ–∫', '–º–∞–ª—å—á–∏–∫–æ–≤', '–æ–¥–µ–∂–¥–∞',
        '—Ä–∞–∑–º–µ—Ä', '—Ü–≤–µ—Ç', '–∞—Ä—Ç–∏–∫—É–ª', '—à—Ç', '—É–ø'
    ]
    
    for w in stop_words:
        title = re.sub(rf'\b{w}\b', '', title)
        
    title = re.sub(r'[^\w\s]', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    
    words = title.split()
    if not words: return "clothing"
    
    result = ' '.join(words[:6])
    return result.capitalize()

# --- Image Tools ---
def is_valid_image_url(url: str) -> bool:
    if not url or not url.startswith('http'): return False
    if any(x in url.lower() for x in ['.svg', '.gif', 'icon', 'logo', 'loader', 'blank']):
        return False
    return True

def analyze_image_score(img: Image.Image, index: int, total_images: int) -> float:
    score = 100.0
    if index > 2: score -= (index * 5)
    
    w, h = img.size
    if w < 300 or h < 300: score -= 50
    aspect = w / h
    if aspect > 1.8 or aspect < 0.4: score -= 30 
    
    gray = img.convert("L")
    edges = gray.filter(ImageFilter.FIND_EDGES)
    edge_density = ImageStat.Stat(edges).mean[0]
    
    if edge_density > 50: score -= 40
    return max(0, score)

# --- MARKETPLACE PARSERS (SYNCHRONOUS) ---

def parse_wildberries(url: str, logger) -> tuple[list, str]:
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è WB.
    –í–µ—Ä—Å–∏—è 3.0: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ (basket-01 ... basket-150) + –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫.
    """
    image_urls = []
    title = None
    nm_id = None
    
    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º ID
    match = re.search(r'catalog/(\d+)', url)
    if match: nm_id = int(match.group(1))
    
    if not nm_id: return [], None

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã vol –∏ part
    vol = nm_id // 100000
    part = nm_id // 1000

    # === –ü–û–ü–´–¢–ö–ê 1: Mobile API (–±—ã—Å—Ç—Ä–æ, –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ) ===
    try:
        api_url = f"https://card.wb.ru/cards/v2/detail?appType=1&curr=rub&dest=-1257786&spp=30&nm={nm_id}"
        headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
            "Accept": "*/*"
        }
        resp = requests.get(api_url, headers=headers, timeout=3)
        if resp.status_code == 200:
            data = resp.json()
            products = data.get('data', {}).get('products', [])
            if products:
                prod = products[0]
                title = prod.get('name')
                logger.info(f"‚úÖ WB API Found Title: {title}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è WB API Title fetch failed: {e}")

    # === –ü–û–ü–´–¢–ö–ê 2: –ü–æ–∏—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (Basket Hunt) ===
    # WB —Ä–∞–∑–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã –ø–æ —Å–µ—Ä–≤–µ—Ä–∞–º basket-01 ... basket-150+.
    # –ú—ã –Ω–µ –∑–Ω–∞–µ–º —Ç–æ—á–Ω—ã–π —Å–µ—Ä–≤–µ—Ä, –ø–æ—ç—Ç–æ–º—É –ø–∏–Ω–≥—É–µ–º –∏—Ö –≤—Å–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
    
    found_host = None
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ (—Å –∑–∞–ø–∞—Å–æ–º –¥–æ 150)
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ (–≤—ã—Å–æ–∫–∏–µ –Ω–æ–º–µ—Ä–∞), —Ç–∞–∫ –∫–∞–∫ ID —Ç–æ–≤–∞—Ä–∞ –±–æ–ª—å—à–æ–π
    hosts = [f"basket-{i:02d}.wbbasket.ru" for i in range(1, 151)]
    hosts.reverse() # –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å –∫–æ–Ω—Ü–∞ (–¥–ª—è –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ —ç—Ç–æ –±—ã—Å—Ç—Ä–µ–µ)

    def check_host(host):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ 1-–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ (—Å–∞–º–∞—è –ª–µ–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        test_url = f"https://{host}/vol{vol}/part{part}/{nm_id}/images/big/1.webp"
        try:
            # –¢–∞–π–º–∞—É—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω (0.4—Å), –∏–Ω–∞—á–µ –±—É–¥–µ–º –∂–¥–∞—Ç—å –≤–µ—á–Ω–æ
            r = requests.head(test_url, timeout=0.4)
            if r.status_code == 200:
                return host
        except:
            pass
        return None

    # –ó–∞–ø—É—Å–∫–∞–µ–º 20 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å 150 —Å–µ—Ä–≤–µ—Ä–æ–≤ –∑–∞ –ø–∞—Ä—É —Å–µ–∫—É–Ω–¥
    logger.info(f"üîç Hunting for image server (ID: {nm_id})...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        future_to_host = {executor.submit(check_host, h): h for h in hosts}
        
        for future in concurrent.futures.as_completed(future_to_host):
            result = future.result()
            if result:
                found_host = result
                # –ö–∞–∫ —Ç–æ–ª—å–∫–æ –Ω–∞—à–ª–∏ —Ä–∞–±–æ—á–∏–π —Å–µ—Ä–≤–µ—Ä ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
                executor.shutdown(wait=False, cancel_futures=True)
                break

    if found_host:
        logger.info(f"‚úÖ Image Server Found: {found_host}")
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ 12 —Ñ–æ—Ç–æ
        for i in range(1, 13):
            image_urls.append(f"https://{found_host}/vol{vol}/part{part}/{nm_id}/images/big/{i}.webp")
            
        return image_urls, title
    else:
        logger.warning(f"‚ùå Failed to find image server for {nm_id} (Checked baskets 01-150)")

    # === –ü–û–ü–´–¢–ö–ê 3: Fallback (–µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ—Ä–∑–∏–Ω—É) ===
    return parse_generic_json_ld(url, logger)

def parse_generic_json_ld(url: str, logger) -> tuple[list, str]:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (JSON-LD / OG)"""
    image_urls = []
    title = None
    
    try:
        resp = crequests.get(
            url, 
            impersonate="chrome120", 
            headers={"Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8"},
            timeout=10
        )
        
        if resp.status_code != 200: return [], None
        
        soup = BeautifulSoup(resp.content, "lxml")
        
        # A. JSON-LD
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, list): data = data[0]
                
                if isinstance(data, dict):
                    if not title and 'name' in data: title = data['name']
                    if 'image' in data:
                        imgs = data['image']
                        if isinstance(imgs, str): image_urls.append(imgs)
                        elif isinstance(imgs, list): image_urls.extend(imgs)
            except: pass

        # B. Open Graph
        if not title:
            og_title = soup.find("meta", property="og:title")
            if og_title: title = og_title.get("content")
            
        if not image_urls:
            og_img = soup.find("meta", property="og:image")
            if og_img: image_urls.append(og_img.get("content"))

        # C. Title Tag
        if not title:
            if soup.title: title = soup.title.string
            elif soup.find("h1"): title = soup.find("h1").get_text(strip=True)

        # D. HTML Scraping
        if len(image_urls) < 2:
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if is_valid_image_url(src):
                    if 'icon' not in src and 'logo' not in src:
                        image_urls.append(src)

    except Exception as e:
        logger.error(f"‚ùå Generic Parser Error: {e}")
        
    image_urls = list(dict.fromkeys(image_urls))
    return image_urls[:15], title

# === MAIN CONTROLLER (SYNCHRONOUS) ===

def get_marketplace_data(url: str):
    """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä (–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π)"""
    logger.info(f"üåê Processing URL: {url}")
    
    if "wildberries" in url or "wb.ru" in url:
        return parse_wildberries(url, logger)
    elif "ozon" in url or "lamoda" in url or "aliexpress" in url:
        return parse_generic_json_ld(url, logger)
    else:
        return parse_generic_json_ld(url, logger)

def download_image_bytes(image_url: str) -> bytes:
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.google.com/'
    }
    try:
        resp = requests.get(image_url, headers=headers, timeout=15)
        if resp.status_code == 200:
            return resp.content
    except Exception as e:
        logger.warning(f"Download failed {image_url}: {e}")
    return None

# --- API ENDPOINTS ---

@router.post("/add-marketplace-with-variants")
async def add_marketplace_with_variants(
    payload: ItemUrlPayload, 
    db: Session = Depends(get_db), 
    user_id: int = Depends(get_current_user_id)
):
    loop = asyncio.get_event_loop()
    
    # 1. –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ (–í –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ)
    try:
        # –í–ê–ñ–ù–û: –í—ã–∑—ã–≤–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é get_marketplace_data
        image_urls, full_title = await loop.run_in_executor(
            None, 
            lambda: get_marketplace_data(payload.url)
        )
    except Exception as e:
        logger.error(f"‚ùå Parser crashed: {e}")
        raise HTTPException(400, f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–∫–∏: {str(e)}")

    if not image_urls:
        logger.warning(f"‚ùå No images found for {payload.url}")
        raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞.")

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ CLIP
    raw_name = payload.name if payload.name else (full_title if full_title else "clothing")
    clip_prompt = extract_smart_title(raw_name)
    logger.info(f"üß† CLIP Prompt: '{clip_prompt}'")

    # 3. –ê–Ω–∞–ª–∏–∑ (–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞)
    temp_id = uuid.uuid4().hex
    candidates = []
    process_urls = image_urls[:10]
    
    for idx, img_url in enumerate(process_urls):
        try:
            file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(img_url))
            if not file_bytes: continue
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è RGBA -> RGB
            img = Image.open(BytesIO(file_bytes))
            if img.mode != 'RGB':
                bg = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode in ('RGBA', 'LA'):
                    bg.paste(img, mask=img.split()[-1])
                else:
                    bg.paste(img)
                img = bg

            heuristic_score = analyze_image_score(img, idx, len(process_urls))
            
            clip_score = 0.0
            if CLIP_AVAILABLE and heuristic_score > 20: 
                clip_score = await loop.run_in_executor(
                    None,
                    lambda: rate_image_relevance(img, clip_prompt)
                )
            
            final_score = (heuristic_score * 0.3) + (clip_score * 0.7)
            
            preview_img = img.copy()
            preview_img.thumbnail((400, 400))
            out = BytesIO()
            preview_img.save(out, format='JPEG', quality=85)
            
            candidates.append({
                "score": final_score,
                "original_url": img_url,
                "preview_bytes": out.getvalue(),
                "original_idx": idx
            })
            
            logger.info(f"Img {idx+1}: Score={final_score:.1f} (CLIP={clip_score:.1f})")
            
        except Exception as e:
            logger.warning(f"Skipping img {idx}: {e}")

    # 4. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –æ—Ç–≤–µ—Ç
    candidates.sort(key=lambda x: x["score"], reverse=True)
    top_candidates = candidates[:4]
    top_candidates.sort(key=lambda x: x["original_idx"])
    
    variant_previews = {}
    variant_full_urls = {}
    
    for cand in top_candidates:
        v_key = f"v_{cand['original_idx']}"
        fname = f"prev_{temp_id}_{v_key}.jpg"
        url = save_image(fname, cand['preview_bytes'])
        
        variant_previews[v_key] = url
        variant_full_urls[v_key] = cand['original_url']

    if not variant_previews:
         raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

    VARIANTS_STORAGE[temp_id] = {
        "image_urls": variant_full_urls,
        "previews": variant_previews,
        "user_id": user_id,
        "created_at": datetime.utcnow()
    }
    
    display_name = full_title if full_title else "–ù–æ–≤–∞—è –≤–µ—â—å"
    if len(display_name) > 60: display_name = display_name[:57] + "..."

    return {
        "temp_id": temp_id,
        "suggested_name": display_name,
        "variants": variant_previews,
        "total_images": len(variant_previews)
    }

@router.post("/select-variant", response_model=ItemResponse)
async def select_variant(
    payload: SelectVariantPayload,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    if payload.temp_id not in VARIANTS_STORAGE:
        raise HTTPException(404, "–í—Ä–µ–º—è —Å–µ—Å—Å–∏–∏ –∏—Å—Ç–µ–∫–ª–æ")
    
    data = VARIANTS_STORAGE[payload.temp_id]
    if data["user_id"] != user_id: raise HTTPException(403, "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞")
    
    target_url = data["image_urls"].get(payload.selected_variant)
    if not target_url: raise HTTPException(400, "–ù–µ–≤–µ—Ä–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç")
    
    loop = asyncio.get_event_loop()
    file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(target_url))
    
    if not file_bytes: raise HTTPException(400, "–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞")
    
    img = Image.open(BytesIO(file_bytes))
    if img.mode != 'RGB': img = img.convert('RGB')
    
    out = BytesIO()
    img.save(out, format='JPEG', quality=95)
    
    fname = f"item_{uuid.uuid4().hex}.jpg"
    final_url = save_image(fname, out.getvalue())
    
    # Cleanup
    for p_url in data["previews"].values():
        try: delete_image(p_url)
        except: pass
    del VARIANTS_STORAGE[payload.temp_id]
    
    item = WardrobeItem(
        user_id=user_id,
        name=payload.name,
        item_type="marketplace",
        image_url=final_url,
        created_at=datetime.utcnow()
    )
    db.add(item); db.commit(); db.refresh(item)
    return item

@router.get("/items", response_model=list[ItemResponse]) 
def get_wardrobe_items(user_id: int = Depends(get_current_user_id), db: Session = Depends(get_db)):
    items = db.query(WardrobeItem).filter(WardrobeItem.user_id == user_id).order_by(WardrobeItem.created_at.desc()).all()
    return items if items else []

@router.delete("/delete")
def delete_item(item_id: int, db: Session = Depends(get_db), user_id: int = Depends(get_current_user_id)):
    item = db.query(WardrobeItem).filter(WardrobeItem.id == item_id, WardrobeItem.user_id == user_id).first()
    if not item: raise HTTPException(404, "Not found")
    try: delete_image(item.image_url)
    except: pass
    db.delete(item); db.commit()
    return {"status": "success"}

