import os
import uuid
import time
import asyncio
import re
import logging
import json # <--- –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è JSON-LD
from datetime import datetime
from io import BytesIO
from PIL import Image, ImageStat, ImageFilter

# requests - –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
import requests
import concurrent.futures
from curl_cffi import requests as crequests
from bs4 import BeautifulSoup

from fastapi import APIRouter, Depends, UploadFile, HTTPException, File, Form
from pydantic import BaseModel
from sqlalchemy.orm import Session

from database import get_db
from models import WardrobeItem
from utils.storage import delete_image, save_image
from utils.validators import validate_name
from .dependencies import get_current_user_id

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LOGGER –°–ù–ê–ß–ê–õ–ê ===
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === –ò–ú–ü–û–†–¢ –ù–û–í–´–• –ú–û–î–£–õ–ï–ô ===
CLIP_AVAILABLE = False
IMAGE_PROCESSOR_AVAILABLE = False

try:
    from utils.clip_client import clip_check_clothing, rate_image_relevance
    CLIP_AVAILABLE = True
    logger.info("‚úÖ CLIP client module loaded")
except ImportError:
    CLIP_AVAILABLE = False
    def rate_image_relevance(img, name): return 50.0

try:
    from utils.image_processor import generate_image_variants, convert_variant_to_bytes
    IMAGE_PROCESSOR_AVAILABLE = True
    logger.info("‚úÖ Image processor module loaded")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Image processor not available: {e}")
    # –ó–∞–≥–ª—É—à–∫–∏
    def generate_image_variants(img, output_size=800):
        return {"original": img}
    def convert_variant_to_bytes(img, format="JPEG", quality=85):
        output = BytesIO()
        img.save(output, format=format, quality=quality, optimize=True)
        return output.getvalue()

router = APIRouter(tags=["Wardrobe"])

# --- Models ---
class ItemUrlPayload(BaseModel):
    name: str
    url: str

class ItemResponse(BaseModel):
    id: int
    name: str
    image_url: str
    item_type: str
    created_at: datetime
    class Config:
        from_attributes = True

class SelectVariantPayload(BaseModel):
    temp_id: str
    selected_variant: str
    name: str

VARIANTS_STORAGE = {}

# --- Helpers ---
def validate_image_bytes(file_bytes: bytes):
    MAX_SIZE_MB = 10
    if len(file_bytes) > MAX_SIZE_MB * 1024 * 1024:
        return False, f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ > {MAX_SIZE_MB} –ú–ë."
    try:
        img = Image.open(BytesIO(file_bytes))
        img.verify()
        if img.format not in ['JPEG', 'PNG', 'GIF', 'WEBP']:
             return False, "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–æ—Ç–æ."
    except Exception:
        return False, "–§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–æ—Ç–æ."
    return True, None

def analyze_image_score(img: Image.Image, index: int, total_images: int) -> float:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≥–∞—Ä–¥–µ—Ä–æ–±–∞ (0-100).
    """
    score = 100.0
    
    # 1. –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–∑–∏—Ü–∏—é (WB —Å—Ç–∞–≤–∏—Ç –ª—É—á—à–∏–µ —Ñ–æ—Ç–æ –ø–µ—Ä–≤—ã–º–∏)
    if index > 2:
        score -= (index * 5)
    
    # 2. –®—Ç—Ä–∞—Ñ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ñ–æ—Ç–æ (—Ç–∞–±–ª–∏—Ü—ã —Ä–∞–∑–º–µ—Ä–æ–≤)
    if index >= total_images - 1 and total_images > 3:
        score -= 20

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á/–± –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    gray = img.convert("L")
    
    # 3. –î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∞–±–ª–∏—Ü –∏ —Ç–µ–∫—Å—Ç–∞
    edges = gray.filter(ImageFilter.FIND_EDGES)
    edge_stat = ImageStat.Stat(edges)
    edge_density = edge_stat.mean[0]
    
    # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ª–∏–Ω–∏–π (—Ç–µ–∫—Å—Ç, —Ç–∞–±–ª–∏—Ü–∞) -> —à—Ç—Ä–∞—Ñ
    if edge_density > 50: 
        score -= 40
        logger.info(f"üìâ Image {index+1}: High edge density ({edge_density:.1f}) -> Likely table")
        
    return score

def find_wb_image_url(nm_id: int) -> str:
    """–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π WB –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö"""
    vol = nm_id // 100000
    part = nm_id // 1000
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ö–æ—Å—Ç–æ–≤
    hosts = [f"basket-{i:02d}.wbbasket.ru" for i in range(1, 75)]
    
    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å URL –±—ã—Å—Ç—Ä–æ, –µ—Å–ª–∏ –∑–Ω–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
    # (–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –æ–±—ã—á–Ω–æ –Ω–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã –ª–µ–∂–∞—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö, —Å—Ç–∞—Ä—ã–µ –Ω–∞ –ø–µ—Ä–≤—ã—Ö)
    
    url_templates = ["https://{host}/vol{vol}/part{part}/{nm_id}/images/big/1.webp"]

    def check_url(url):
        try:
            resp = requests.head(url, headers=headers, timeout=0.5)
            if resp.status_code == 200: return url
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        all_urls = []
        for host in hosts:
            all_urls.append(url_templates[0].format(host=host, vol=vol, part=part, nm_id=nm_id))
        
        future_to_url = {executor.submit(check_url, url): url for url in all_urls}
        for future in concurrent.futures.as_completed(future_to_url):
            result = future.result()
            if result:
                executor.shutdown(wait=False, cancel_futures=True)
                return result
    return None
    
def extract_smart_title(full_title: str) -> str:
    """
    –ß–∏—Å—Ç–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è CLIP.
    –î–µ–ª–∞–µ–º –µ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    if not full_title: return "clothing"
    
    # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –±—Ä–∞—É–∑–µ—Ä–∞
    # –ü—Ä–∏–º–µ—Ä: "–ü–ª–∞—Ç—å–µ –∂–µ–Ω—Å–∫–æ–µ –≤–µ—á–µ—Ä–Ω–µ–µ - –∫—É–ø–∏—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–µ Wildberries"
    cleanup_patterns = [
        r'[-|‚Äì].*wildberries.*',  # –í—Å–µ —á—Ç–æ –ø–æ—Å–ª–µ —Ç–∏—Ä–µ –ø—Ä–æ WB
        r'[-|‚Äì].*ozon.*',
        r'[-|‚Äì].*lamoda.*',
        r'–∫—É–ø–∏—Ç—å –≤ .*',
        r'–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω.*',
        r'wildberries', 'wb', 'ozon', 'lamoda'
    ]
    
    title = full_title.lower()
    for pat in cleanup_patterns:
        title = re.sub(pat, '', title)

    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç –≤–∏–∑—É–∞–ª
    stop_words = [
        '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–Ω–æ–≤–∏–Ω–∫–∞', '—Ö–∏—Ç',
        '–±—ã—Å—Ç—Ä–∞—è', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è', '–∂–µ–Ω—Å–∫–∏–µ', '–º—É–∂—Å–∫–∏–µ',
        '–¥–ª—è', '–∂–µ–Ω—â–∏–Ω', '–º—É–∂—á–∏–Ω', '–¥–µ–≤–æ—á–µ–∫', '–º–∞–ª—å—á–∏–∫–æ–≤',
        '—Ä–∞–∑–º–µ—Ä', '—Ü–≤–µ—Ç', '–∞—Ä—Ç–∏–∫—É–ª'
    ]
    
    for w in stop_words:
        title = title.replace(f" {w} ", " ")
        
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    title = re.sub(r'[^\w\s]', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–ª–æ–≤, –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–µ
    words = title.split()
    if not words: return "clothing"
    
    result = ' '.join(words[:5])
    return result.capitalize()

def get_marketplace_data(url: str):
    """
    –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ü–ê–†–°–ò–ù–ì.
    –ü—ã—Ç–∞–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ —Ñ–æ—Ç–æ –ª—é–±—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.
    """
    logger.info(f"üåê Processing URL: {url}")
    image_urls = []
    title = None
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è curl_cffi (–∏–º–∏—Ç–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞)
    # –í–∞–∂–Ω–æ –º–µ–Ω—è—Ç—å –≤–µ—Ä—Å–∏–∏ Chrome, —á—Ç–æ–±—ã –Ω–µ –ø–∞–ª–∏–ª–∏
    browser_params = {
        "impersonate": "chrome120",
        "headers": {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "max-age=0",
            "Referer": "https://www.google.com/"
        },
        "timeout": 15
    }

    # === 1. –ü–´–¢–ê–ï–ú–°–Ø –°–ö–ê–ß–ê–¢–¨ HTML (–°–ê–ú–´–ô –ù–ê–î–ï–ñ–ù–´–ô –ú–ï–¢–û–î –î–õ–Ø –ù–ê–ó–í–ê–ù–ò–Ø) ===
    soup = None
    try:
        resp = crequests.get(url, **browser_params)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, "lxml")
            
            # --- –ò–©–ï–ú –ù–ê–ó–í–ê–ù–ò–ï –í–ï–ó–î–ï ---
            
            # A. JSON-LD (–°–∫—Ä—ã—Ç–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–ª—è Google) - –°–∞–º–æ–µ —Ç–æ—á–Ω–æ–µ
            if not title:
                scripts = soup.find_all('script', type='application/ld+json')
                for script in scripts:
                    try:
                        data = json.loads(script.string)
                        # –ò–Ω–æ–≥–¥–∞ —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –∏–Ω–æ–≥–¥–∞ —Å–ª–æ–≤–∞—Ä—å
                        if isinstance(data, list): data = data[0]
                        
                        if isinstance(data, dict):
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ö–µ–º—ã
                            if 'name' in data:
                                title = data['name']
                                logger.info(f"‚úÖ Found title in JSON-LD: {title}")
                            
                            # –ó–∞–æ–¥–Ω–æ –∏—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É —Ç–∞–º –∂–µ
                            if 'image' in data:
                                imgs = data['image']
                                if isinstance(imgs, str): image_urls.append(imgs)
                                elif isinstance(imgs, list): image_urls.extend(imgs)
                    except: pass

            # B. Open Graph (og:title)
            if not title:
                og = soup.find("meta", property="og:title")
                if og: 
                    title = og.get("content")
                    logger.info(f"‚úÖ Found title in OG: {title}")

            # C. –¢–µ–≥ <title> (–ï—Å—Ç—å –≤—Å–µ–≥–¥–∞!)
            if not title and soup.title:
                raw_title = soup.title.string
                # –û–±—ã—á–Ω–æ —Ç–∞–º "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ - –∫—É–ø–∏—Ç—å –Ω–∞ Wildberries..."
                # –ú—ã –ø–æ—á–∏—Å—Ç–∏–º —ç—Ç–æ –ø–æ–∑–∂–µ –≤ extract_smart_title
                title = raw_title
                logger.info(f"‚úÖ Found title in <title> tag: {title}")

            # D. H1 (–ö–ª–∞—Å—Å–∏–∫–∞)
            if not title:
                h1 = soup.find("h1")
                if h1: title = h1.get_text(strip=True)

    except Exception as e:
        logger.error(f"‚ö†Ô∏è HTML parsing failed: {e}")

    # === 2. –°–ü–ï–¶–ò–§–ò–ö–ê WILDBERRIES (API + –ö–ê–†–¢–ò–ù–ö–ò) ===
    if "wildberries" in url or "wb.ru" in url:
        try:
            match = re.search(r'catalog/(\d+)', url)
            if match:
                nm_id = int(match.group(1))
                
                # –ü–æ–ø—ã—Ç–∫–∞ –¥–æ—Å—Ç–∞—Ç—å —Ñ–æ—Ç–æ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—Å—ã–ª–æ–∫ (—ç—Ç–æ –±—ã—Å—Ç—Ä–µ–µ API)
                # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Å–µ—Ä–≤–µ—Ä–∞ basket-01 ... basket-20 (–Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤)
                # –ò basket-01 ... basket-15 –¥–ª—è —Å—Ç–∞—Ä—ã—Ö. 
                # –í–æ–∑—å–º–µ–º —à–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω, –Ω–æ —Å –±—ã—Å—Ç—Ä—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º.
                
                # –ï—Å–ª–∏ –º—ã –Ω–∞—à–ª–∏ —Ñ–æ—Ç–æ –≤ JSON-LD (–≤—ã—à–µ), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö.
                # –ï—Å–ª–∏ –Ω–µ—Ç - –∑–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–±–æ—Ä.
                if not image_urls:
                    base_url = find_wb_image_url(nm_id) # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É –≤–∞—Å —É–∂–µ –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ
                    if base_url:
                        host_match = re.search(r'basket-\d+\.wbbasket\.ru', base_url)
                        if host_match:
                            host = host_match.group(0)
                            vol = nm_id // 100000
                            part = nm_id // 1000
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 10 —Å—Å—ã–ª–æ–∫
                            for i in range(1, 11):
                                image_urls.append(f"https://{host}/vol{vol}/part{part}/{nm_id}/images/big/{i}.webp")
                
                # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –≤—Å—ë –µ—â–µ –Ω–µ—Ç (HTML –Ω–µ –æ—Ç–¥–∞–ª—Å—è), –ø—Ä–æ–±—É–µ–º API –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–∞–¥–µ–∂–¥—É
                if not title:
                    card_url = f"https://card.wb.ru/cards/v2/detail?appType=1&curr=rub&dest=-1&spp=30&nm={nm_id}"
                    try:
                        res = requests.get(card_url, timeout=3)
                        if res.status_code == 200:
                            data = res.json()
                            prod = data.get('data', {}).get('products', [{}])[0]
                            title = prod.get('name')
                    except: pass
                    
        except Exception as e:
            logger.error(f"‚ùå WB Logic Error: {e}")

    # === 3. –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê ===
    
    # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ HTML –Ω–∞—à–µ–ª –∫–∞—Ä—Ç–∏–Ω–∫–∏ (Ozon/Lamoda), —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏—Ö
    # –£–±–∏—Ä–∞–µ–º –º–µ–ª–∫–∏–µ –∏–∫–æ–Ω–∫–∏ –∏ –¥—É–±–ª–∏
    unique_urls = []
    for u in image_urls:
        if u not in unique_urls and u.startswith('http'):
            unique_urls.append(u)
    
    final_title = title.strip() if title else None
    
    # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ö–æ—Ç—è –±—ã "–¢–æ–≤–∞—Ä"
    if not final_title:
        logger.warning("‚ùå No title found anywhere. Using fallback.")
    
    return unique_urls, final_title

def download_image_bytes(image_url: str) -> bytes:
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å User-Agent"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.wildberries.ru/' # –ß–∞—Å—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç
    }
    try:
        resp = requests.get(image_url, headers=headers, timeout=15)
        if resp.status_code == 200:
            return resp.content
    except Exception as e:
        logger.warning(f"Download failed {image_url}: {e}")
    return None

# --- Main Endpoints ---

@router.post("/add-marketplace-with-variants")
async def add_marketplace_with_variants(
    payload: ItemUrlPayload, 
    db: Session = Depends(get_db), 
    user_id: int = Depends(get_current_user_id)
):
    loop = asyncio.get_event_loop()
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    image_urls, full_title = await loop.run_in_executor(
        None, 
        lambda: get_marketplace_data(payload.url)
    )
    
    if not image_urls:
        raise HTTPException(400, "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Å—ã–ª–∫—É.")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è –∏–∑ payload –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ, –∏–Ω–∞—á–µ "clothing"
    # –ù–æ –¥–ª—è CLIP –Ω–∞–º –Ω—É–∂–Ω–æ –æ—á–∏—â–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    raw_name = payload.name if payload.name else (full_title if full_title else "clothing")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è CLIP (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ!)
    clip_prompt = extract_smart_title(raw_name)
    logger.info(f"üß† CLIP Search Prompt: '{clip_prompt}' (Original: {raw_name[:30]}...)")

    image_urls = image_urls[:10] # –ë–µ—Ä–µ–º —Ç–æ–ø-10
    temp_id = uuid.uuid4().hex
    candidates = []

    # 2. –ê–Ω–∞–ª–∏–∑
    for idx, img_url in enumerate(image_urls):
        try:
            file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(img_url))
            if not file_bytes: continue
            
            # --- FIX RGBA HERE ---
            img = Image.open(BytesIO(file_bytes))
            # –°–†–ê–ó–£ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ "cannot write mode RGBA as JPEG" –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º
            if img.mode != 'RGB':
                bg = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode in ('RGBA', 'LA'):
                    bg.paste(img, mask=img.split()[-1])
                else:
                    bg.paste(img)
                img = bg

            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞
            heuristic_score = analyze_image_score(img, idx, len(image_urls))
            
            # CLIP
            clip_score = 0.0
            if CLIP_AVAILABLE and heuristic_score > 20:
                clip_score = await loop.run_in_executor(
                    None,
                    lambda: rate_image_relevance(img, clip_prompt) # <-- –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                )
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: CLIP –≤–∞–∂–Ω–µ–µ (70%), —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è (30%)
            # –ï—Å–ª–∏ CLIP –Ω–∞—à–µ–ª —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –æ–Ω–æ –ø–µ—Ä–µ–≤–µ—Å–∏—Ç –ø–æ—Ä—è–¥–æ–∫ —Ñ–æ—Ç–æ
            final_score = (heuristic_score * 0.3) + (clip_score * 0.7)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–≤—å—é
            preview_img = img.copy()
            preview_img.thumbnail((400, 400)) # –ß—É—Ç—å –±–æ–ª—å—à–µ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            
            out = BytesIO()
            preview_img.save(out, format='JPEG', quality=80)
            preview_bytes = out.getvalue()
            
            candidates.append({
                "score": final_score,
                "original_url": img_url,
                "preview_bytes": preview_bytes,
                "original_idx": idx
            })
            
            logger.info(f"Img {idx+1}: Score={final_score:.1f} (CLIP={clip_score:.1f})")
            
        except Exception as e:
            logger.warning(f"Error processing img {idx}: {e}")

    # 3. –í—ã–±–æ—Ä –ª—É—á—à–∏—Ö
    candidates.sort(key=lambda x: x["score"], reverse=True)
    top_candidates = candidates[:4]
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ø –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É (—á—Ç–æ–±—ã –Ω–µ –ø—Ä—ã–≥–∞–ª–∏ —Ü–≤–µ—Ç–∞)
    top_candidates.sort(key=lambda x: x["original_idx"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–≤—å—é
    variant_previews = {}
    variant_full_urls = {}
    
    for cand in top_candidates:
        v_key = f"v_{cand['original_idx']}"
        fname = f"prev_{temp_id}_{v_key}.jpg"
        url = save_image(fname, cand["preview_bytes"])
        
        variant_previews[v_key] = url
        variant_full_urls[v_key] = cand["original_url"]

    VARIANTS_STORAGE[temp_id] = {
        "image_urls": variant_full_urls,
        "previews": variant_previews,
        "user_id": user_id,
        "created_at": datetime.utcnow()
    }
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    display_name = full_title if full_title else "–ù–æ–≤–∞—è –≤–µ—â—å"
    if len(display_name) > 50: display_name = display_name[:47] + "..."

    return {
        "temp_id": temp_id,
        "suggested_name": display_name,
        "variants": variant_previews,
        "total_images": len(variant_previews)
    }

@router.post("/select-variant", response_model=ItemResponse)
async def select_variant(
    payload: SelectVariantPayload,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    if payload.temp_id not in VARIANTS_STORAGE:
        raise HTTPException(404, "Session expired")
    
    data = VARIANTS_STORAGE[payload.temp_id]
    if data["user_id"] != user_id: raise HTTPException(403, "Access denied")
    
    target_url = data["image_urls"].get(payload.selected_variant)
    if not target_url: raise HTTPException(400, "Invalid variant")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    loop = asyncio.get_event_loop()
    file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(target_url))
    
    if not file_bytes: raise HTTPException(400, "Failed to download original")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    img = Image.open(BytesIO(file_bytes))
    if img.mode != 'RGB': img = img.convert('RGB')
    
    out = BytesIO()
    img.save(out, format='JPEG', quality=90)
    final_bytes = out.getvalue()
    
    fname = f"item_{uuid.uuid4().hex}.jpg"
    final_url = save_image(fname, final_bytes)
    
    # –ß–∏—Å—Ç–∏–º –ø—Ä–µ–≤—å—é
    for p_url in data["previews"].values():
        try: delete_image(p_url)
        except: pass
    del VARIANTS_STORAGE[payload.temp_id]
    
    item = WardrobeItem(
        user_id=user_id,
        name=payload.name,
        item_type="marketplace",
        image_url=final_url,
        created_at=datetime.utcnow()
    )
    db.add(item); db.commit(); db.refresh(item)
    return item

# (–û—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–æ—É—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: items, delete –∏ —Ç.–¥.)
# ...
@router.get("/items", response_model=list[ItemResponse]) 
def get_wardrobe_items(user_id: int = Depends(get_current_user_id), db: Session = Depends(get_db)):
    items = db.query(WardrobeItem).filter(WardrobeItem.user_id == user_id).order_by(WardrobeItem.created_at.desc()).all()
    return items if items else []

@router.delete("/delete")
def delete_item(item_id: int, db: Session = Depends(get_db), user_id: int = Depends(get_current_user_id)):
    item = db.query(WardrobeItem).filter(WardrobeItem.id == item_id, WardrobeItem.user_id == user_id).first()
    if not item: raise HTTPException(404, "Not found")
    try: delete_image(item.image_url)
    except: pass
    db.delete(item); db.commit()
    return {"status": "success"}

