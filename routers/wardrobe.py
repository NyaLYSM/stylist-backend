import os
import uuid
import time
import asyncio
import re
import logging
import json # <--- –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è JSON-LD
from datetime import datetime
from io import BytesIO
from PIL import Image, ImageStat, ImageFilter

# requests - –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
import requests
import concurrent.futures
from curl_cffi import requests as crequests
from bs4 import BeautifulSoup

from fastapi import APIRouter, Depends, UploadFile, HTTPException, File, Form
from pydantic import BaseModel
from sqlalchemy.orm import Session

from database import get_db
from models import WardrobeItem
from utils.storage import delete_image, save_image
from utils.validators import validate_name
from .dependencies import get_current_user_id

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LOGGER –°–ù–ê–ß–ê–õ–ê ===
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === –ò–ú–ü–û–†–¢ –ù–û–í–´–• –ú–û–î–£–õ–ï–ô ===
CLIP_AVAILABLE = False
IMAGE_PROCESSOR_AVAILABLE = False

try:
    from utils.clip_client import clip_check_clothing, rate_image_relevance
    CLIP_AVAILABLE = True
    logger.info("‚úÖ CLIP client module loaded")
except ImportError:
    CLIP_AVAILABLE = False
    def rate_image_relevance(img, name): return 50.0

try:
    from utils.image_processor import generate_image_variants, convert_variant_to_bytes
    IMAGE_PROCESSOR_AVAILABLE = True
    logger.info("‚úÖ Image processor module loaded")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Image processor not available: {e}")
    # –ó–∞–≥–ª—É—à–∫–∏
    def generate_image_variants(img, output_size=800):
        return {"original": img}
    def convert_variant_to_bytes(img, format="JPEG", quality=85):
        output = BytesIO()
        img.save(output, format=format, quality=quality, optimize=True)
        return output.getvalue()

router = APIRouter(tags=["Wardrobe"])

# --- Models ---
class ItemUrlPayload(BaseModel):
    name: str
    url: str

class ItemResponse(BaseModel):
    id: int
    name: str
    image_url: str
    item_type: str
    created_at: datetime
    class Config:
        from_attributes = True

class SelectVariantPayload(BaseModel):
    temp_id: str
    selected_variant: str
    name: str

VARIANTS_STORAGE = {}

# --- Helpers ---
def validate_image_bytes(file_bytes: bytes):
    MAX_SIZE_MB = 10
    if len(file_bytes) > MAX_SIZE_MB * 1024 * 1024:
        return False, f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ > {MAX_SIZE_MB} –ú–ë."
    try:
        img = Image.open(BytesIO(file_bytes))
        img.verify()
        if img.format not in ['JPEG', 'PNG', 'GIF', 'WEBP']:
             return False, "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–æ—Ç–æ."
    except Exception:
        return False, "–§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–æ—Ç–æ."
    return True, None

def analyze_image_score(img: Image.Image, index: int, total_images: int) -> float:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≥–∞—Ä–¥–µ—Ä–æ–±–∞ (0-100).
    """
    score = 100.0
    
    # 1. –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–∑–∏—Ü–∏—é (WB —Å—Ç–∞–≤–∏—Ç –ª—É—á—à–∏–µ —Ñ–æ—Ç–æ –ø–µ—Ä–≤—ã–º–∏)
    if index > 2:
        score -= (index * 5)
    
    # 2. –®—Ç—Ä–∞—Ñ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ñ–æ—Ç–æ (—Ç–∞–±–ª–∏—Ü—ã —Ä–∞–∑–º–µ—Ä–æ–≤)
    if index >= total_images - 1 and total_images > 3:
        score -= 20

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á/–± –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    gray = img.convert("L")
    
    # 3. –î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∞–±–ª–∏—Ü –∏ —Ç–µ–∫—Å—Ç–∞
    edges = gray.filter(ImageFilter.FIND_EDGES)
    edge_stat = ImageStat.Stat(edges)
    edge_density = edge_stat.mean[0]
    
    # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ª–∏–Ω–∏–π (—Ç–µ–∫—Å—Ç, —Ç–∞–±–ª–∏—Ü–∞) -> —à—Ç—Ä–∞—Ñ
    if edge_density > 50: 
        score -= 40
        logger.info(f"üìâ Image {index+1}: High edge density ({edge_density:.1f}) -> Likely table")
        
    return score

def find_wb_image_url(nm_id: int) -> str:
    """–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π WB –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö"""
    vol = nm_id // 100000
    part = nm_id // 1000
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ö–æ—Å—Ç–æ–≤
    hosts = [f"basket-{i:02d}.wbbasket.ru" for i in range(1, 75)]
    
    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å URL –±—ã—Å—Ç—Ä–æ, –µ—Å–ª–∏ –∑–Ω–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
    # (–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –æ–±—ã—á–Ω–æ –Ω–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã –ª–µ–∂–∞—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö, —Å—Ç–∞—Ä—ã–µ –Ω–∞ –ø–µ—Ä–≤—ã—Ö)
    
    url_templates = ["https://{host}/vol{vol}/part{part}/{nm_id}/images/big/1.webp"]

    def check_url(url):
        try:
            resp = requests.head(url, headers=headers, timeout=0.5)
            if resp.status_code == 200: return url
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        all_urls = []
        for host in hosts:
            all_urls.append(url_templates[0].format(host=host, vol=vol, part=part, nm_id=nm_id))
        
        future_to_url = {executor.submit(check_url, url): url for url in all_urls}
        for future in concurrent.futures.as_completed(future_to_url):
            result = future.result()
            if result:
                executor.shutdown(wait=False, cancel_futures=True)
                return result
    return None
    
def extract_smart_title(full_title: str) -> str:
    """–ß–∏—Å—Ç–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è CLIP"""
    if not full_title: return "clothing"
    
    title = full_title.lower()
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
    title = re.sub(r'[\/\-]', ' ', title) # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–µ—à–∏ –∏ –¥–µ—Ñ–∏—Å—ã –ø—Ä–æ–±–µ–ª–∞–º–∏
    
    stop_words = [
        'wildberries', 'wb', 'ozon', '—Ç–æ–≤–∞—Ä', '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', 
        '–∂–µ–Ω—Å–∫–∏–µ', '–∂–µ–Ω—Å–∫–∞—è', '–º—É–∂—Å–∫–∏–µ', '–º—É–∂—Å–∫–∞—è', '–¥–µ—Ç—Å–∫–∏–µ', 
        '—Ä–∞–∑–º–µ—Ä', '—Ü–≤–µ—Ç', '–Ω–æ–≤–∏–Ω–∫–∞', '—Ö–∏—Ç', '2024', '2025', '2026'
    ]
    
    for w in stop_words:
        title = title.replace(w, '')
        
    # –£–±–∏—Ä–∞–µ–º —Ü–∏—Ñ—Ä—ã (–∞—Ä—Ç–∏–∫—É–ª—ã)
    title = re.sub(r'\b\d+\b', '', title)
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 4 —Å–ª–æ–≤–∞ - –æ–±—ã—á–Ω–æ —Ç–∞–º —Å—É—Ç—å (–Ω–∞–ø—Ä. "–ü–ª–∞—Ç—å–µ –≤–µ—á–µ—Ä–Ω–µ–µ —á–µ—Ä–Ω–æ–µ –≤ –ø–æ–ª")
    words = [w for w in title.split() if len(w) > 2]
    result = ' '.join(words[:4]).strip()
    
    return result.capitalize() if result else "clothing"

def get_marketplace_data(url: str):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞.
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è WB –∏ JSON-LD.
    """
    logger.info(f"üåê Processing URL: {url}")
    image_urls = []
    title = None
    
    # === WILDBERRIES ===
    if "wildberries" in url or "wb.ru" in url:
        try:
            match = re.search(r'catalog/(\d+)', url)
            if match:
                nm_id = int(match.group(1))
                
                # 1. WB API v2 (—Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ dest=-1)
                # dest=-1 —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—á—Ç–∏ –≤–µ–∑–¥–µ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω–æ–π –ª–æ–∫–∞—Ü–∏–∏
                card_url = f"https://card.wb.ru/cards/v2/detail?appType=1&curr=rub&dest=-1&spp=30&nm={nm_id}"
                try:
                    res = requests.get(card_url, timeout=5)
                    if res.status_code == 200:
                        data = res.json()
                        product = data.get('data', {}).get('products', [{}])[0]
                        
                        # –ù–∞–∑–≤–∞–Ω–∏–µ
                        title = product.get('name')
                        if not title: title = product.get('brand', '') + ' ' + product.get('name', '')
                        
                        # –ö–æ–ª-–≤–æ —Ñ–æ—Ç–æ
                        pics_count = product.get('pics', 0)
                        logger.info(f"‚úÖ WB API: Title='{title}', Pics={pics_count}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è WB API failed: {e}")

                # 2. –ï—Å–ª–∏ API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –∏—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –ø–µ—Ä–µ–±–æ—Ä–æ–º
                base_url = find_wb_image_url(nm_id)
                if base_url:
                    host_match = re.search(r'basket-\d+\.wbbasket\.ru', base_url)
                    if host_match:
                        host = host_match.group(0)
                        vol = nm_id // 100000
                        part = nm_id // 1000
                        
                        count = locals().get('pics_count', 12) # –ï—Å–ª–∏ API –æ—Ç–≤–∞–ª–∏–ª–æ—Å—å, –±–µ—Ä–µ–º 12
                        if count == 0: count = 12

                        for i in range(1, count + 1):
                            image_urls.append(f"https://{host}/vol{vol}/part{part}/{nm_id}/images/big/{i}.webp")
            
            # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤—Å–µ –µ—â–µ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ curl_cffi
            if not title:
                try:
                    resp = crequests.get(url, impersonate="chrome120", timeout=8)
                    soup = BeautifulSoup(resp.content, "lxml")
                    
                    # –ü–æ–∏—Å–∫ h1
                    h1 = soup.find("h1")
                    if h1: title = h1.get_text(strip=True)
                except: pass

        except Exception as e:
            logger.error(f"‚ùå WB Error: {e}")

    # === –û–ë–©–ò–ô –ü–ê–†–°–ò–ù–ì (OZON, LAMODA –∏ —Ç.–¥.) ===
    else:
        try:
            resp = crequests.get(url, impersonate="chrome120", timeout=10)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.content, "lxml")
                
                # 1. JSON-LD (–°–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è SEO —Å–∞–π—Ç–æ–≤)
                if not title:
                    scripts = soup.find_all('script', type='application/ld+json')
                    for script in scripts:
                        try:
                            data = json.loads(script.string)
                            # –ò—â–µ–º Product schema
                            if isinstance(data, dict) and data.get('@type') == 'Product':
                                title = data.get('name')
                                # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞ –≤ —Å—Ö–µ–º–µ
                                if 'image' in data:
                                    img = data['image']
                                    if isinstance(img, str): image_urls.append(img)
                                    elif isinstance(img, list): image_urls.extend(img)
                                break
                            # –ò–Ω–æ–≥–¥–∞ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                            elif isinstance(data, list):
                                for item in data:
                                    if item.get('@type') == 'Product':
                                        title = item.get('name')
                                        break
                        except: pass

                # 2. Open Graph
                if not title:
                    og = soup.find("meta", property="og:title")
                    if og: title = og.get("content")
                
                # 3. H1
                if not title:
                    h1 = soup.find("h1")
                    if h1: title = h1.get_text(strip=True)

                # –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ—Ç)
                if not image_urls:
                    for img in soup.find_all('img'):
                        src = img.get('src') or img.get('data-src')
                        if src and src.startswith('http') and ('large' in src or 'big' in src or 'gallery' in src):
                            image_urls.append(src)

        except Exception as e:
            logger.error(f"‚ùå General parser error: {e}")

    final_title = title.strip() if title else None
    return image_urls, final_title

def download_image_bytes(image_url: str) -> bytes:
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å User-Agent"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.wildberries.ru/' # –ß–∞—Å—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç
    }
    try:
        resp = requests.get(image_url, headers=headers, timeout=15)
        if resp.status_code == 200:
            return resp.content
    except Exception as e:
        logger.warning(f"Download failed {image_url}: {e}")
    return None

# --- Main Endpoints ---

@router.post("/add-marketplace-with-variants")
async def add_marketplace_with_variants(
    payload: ItemUrlPayload, 
    db: Session = Depends(get_db), 
    user_id: int = Depends(get_current_user_id)
):
    loop = asyncio.get_event_loop()
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    image_urls, full_title = await loop.run_in_executor(
        None, 
        lambda: get_marketplace_data(payload.url)
    )
    
    if not image_urls:
        raise HTTPException(400, "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Å—ã–ª–∫—É.")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è –∏–∑ payload –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ, –∏–Ω–∞—á–µ "clothing"
    # –ù–æ –¥–ª—è CLIP –Ω–∞–º –Ω—É–∂–Ω–æ –æ—á–∏—â–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    raw_name = payload.name if payload.name else (full_title if full_title else "clothing")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è CLIP (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ!)
    clip_prompt = extract_smart_title(raw_name)
    logger.info(f"üß† CLIP Search Prompt: '{clip_prompt}' (Original: {raw_name[:30]}...)")

    image_urls = image_urls[:10] # –ë–µ—Ä–µ–º —Ç–æ–ø-10
    temp_id = uuid.uuid4().hex
    candidates = []

    # 2. –ê–Ω–∞–ª–∏–∑
    for idx, img_url in enumerate(image_urls):
        try:
            file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(img_url))
            if not file_bytes: continue
            
            # --- FIX RGBA HERE ---
            img = Image.open(BytesIO(file_bytes))
            # –°–†–ê–ó–£ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ "cannot write mode RGBA as JPEG" –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º
            if img.mode != 'RGB':
                bg = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode in ('RGBA', 'LA'):
                    bg.paste(img, mask=img.split()[-1])
                else:
                    bg.paste(img)
                img = bg

            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞
            heuristic_score = analyze_image_score(img, idx, len(image_urls))
            
            # CLIP
            clip_score = 0.0
            if CLIP_AVAILABLE and heuristic_score > 20:
                clip_score = await loop.run_in_executor(
                    None,
                    lambda: rate_image_relevance(img, clip_prompt) # <-- –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                )
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: CLIP –≤–∞–∂–Ω–µ–µ (70%), —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è (30%)
            # –ï—Å–ª–∏ CLIP –Ω–∞—à–µ–ª —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –æ–Ω–æ –ø–µ—Ä–µ–≤–µ—Å–∏—Ç –ø–æ—Ä—è–¥–æ–∫ —Ñ–æ—Ç–æ
            final_score = (heuristic_score * 0.3) + (clip_score * 0.7)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–≤—å—é
            preview_img = img.copy()
            preview_img.thumbnail((400, 400)) # –ß—É—Ç—å –±–æ–ª—å—à–µ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            
            out = BytesIO()
            preview_img.save(out, format='JPEG', quality=80)
            preview_bytes = out.getvalue()
            
            candidates.append({
                "score": final_score,
                "original_url": img_url,
                "preview_bytes": preview_bytes,
                "original_idx": idx
            })
            
            logger.info(f"Img {idx+1}: Score={final_score:.1f} (CLIP={clip_score:.1f})")
            
        except Exception as e:
            logger.warning(f"Error processing img {idx}: {e}")

    # 3. –í—ã–±–æ—Ä –ª—É—á—à–∏—Ö
    candidates.sort(key=lambda x: x["score"], reverse=True)
    top_candidates = candidates[:4]
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ø –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É (—á—Ç–æ–±—ã –Ω–µ –ø—Ä—ã–≥–∞–ª–∏ —Ü–≤–µ—Ç–∞)
    top_candidates.sort(key=lambda x: x["original_idx"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–≤—å—é
    variant_previews = {}
    variant_full_urls = {}
    
    for cand in top_candidates:
        v_key = f"v_{cand['original_idx']}"
        fname = f"prev_{temp_id}_{v_key}.jpg"
        url = save_image(fname, cand["preview_bytes"])
        
        variant_previews[v_key] = url
        variant_full_urls[v_key] = cand["original_url"]

    VARIANTS_STORAGE[temp_id] = {
        "image_urls": variant_full_urls,
        "previews": variant_previews,
        "user_id": user_id,
        "created_at": datetime.utcnow()
    }
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    display_name = full_title if full_title else "–ù–æ–≤–∞—è –≤–µ—â—å"
    if len(display_name) > 50: display_name = display_name[:47] + "..."

    return {
        "temp_id": temp_id,
        "suggested_name": display_name,
        "variants": variant_previews,
        "total_images": len(variant_previews)
    }

@router.post("/select-variant", response_model=ItemResponse)
async def select_variant(
    payload: SelectVariantPayload,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    if payload.temp_id not in VARIANTS_STORAGE:
        raise HTTPException(404, "Session expired")
    
    data = VARIANTS_STORAGE[payload.temp_id]
    if data["user_id"] != user_id: raise HTTPException(403, "Access denied")
    
    target_url = data["image_urls"].get(payload.selected_variant)
    if not target_url: raise HTTPException(400, "Invalid variant")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    loop = asyncio.get_event_loop()
    file_bytes = await loop.run_in_executor(None, lambda: download_image_bytes(target_url))
    
    if not file_bytes: raise HTTPException(400, "Failed to download original")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    img = Image.open(BytesIO(file_bytes))
    if img.mode != 'RGB': img = img.convert('RGB')
    
    out = BytesIO()
    img.save(out, format='JPEG', quality=90)
    final_bytes = out.getvalue()
    
    fname = f"item_{uuid.uuid4().hex}.jpg"
    final_url = save_image(fname, final_bytes)
    
    # –ß–∏—Å—Ç–∏–º –ø—Ä–µ–≤—å—é
    for p_url in data["previews"].values():
        try: delete_image(p_url)
        except: pass
    del VARIANTS_STORAGE[payload.temp_id]
    
    item = WardrobeItem(
        user_id=user_id,
        name=payload.name,
        item_type="marketplace",
        image_url=final_url,
        created_at=datetime.utcnow()
    )
    db.add(item); db.commit(); db.refresh(item)
    return item

# (–û—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–æ—É—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: items, delete –∏ —Ç.–¥.)
# ...
@router.get("/items", response_model=list[ItemResponse]) 
def get_wardrobe_items(user_id: int = Depends(get_current_user_id), db: Session = Depends(get_db)):
    items = db.query(WardrobeItem).filter(WardrobeItem.user_id == user_id).order_by(WardrobeItem.created_at.desc()).all()
    return items if items else []

@router.delete("/delete")
def delete_item(item_id: int, db: Session = Depends(get_db), user_id: int = Depends(get_current_user_id)):
    item = db.query(WardrobeItem).filter(WardrobeItem.id == item_id, WardrobeItem.user_id == user_id).first()
    if not item: raise HTTPException(404, "Not found")
    try: delete_image(item.image_url)
    except: pass
    db.delete(item); db.commit()
    return {"status": "success"}
